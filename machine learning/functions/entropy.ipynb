{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(outcomes):\n",
    "    outcome_counts = Counter(outcomes)\n",
    "    total_outcomes = len(outcomes)\n",
    "\n",
    "    probabilities = [count / total_outcomes for count in outcome_counts.values()]\n",
    "\n",
    "    entropy_value = -sum(p * math.log2(p) for p in probabilities)\n",
    "\n",
    "    return {\n",
    "        \"occurrences\": dict(outcome_counts),\n",
    "        \"probabilities\": dict(zip(outcome_counts.keys(), probabilities)),\n",
    "        \"entropy\": entropy_value,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [3, 2, 5, 1, 4, 6, 3, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_entropy = entropy(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occurrences     {3: 3, 2: 2, 5: 1, 1: 2, 4: 1, 6: 1}\n",
      "probabilities   {3: 0.3, 2: 0.2, 5: 0.1, 1: 0.2, 4: 0.1, 6: 0.1}\n",
      "Entropy         2.4464393446710155\n"
     ]
    }
   ],
   "source": [
    "print(f'{'occurrences':<15} {outcomes_entropy['occurrences']}')\n",
    "print(f'{'probabilities':<15} {outcomes_entropy['probabilities']}')\n",
    "print(f'{'Entropy':<15} {outcomes_entropy['entropy']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The entropy value of approximately 2.446 for the given sequence of outcomes indicates the average amount of uncertainty or information content associated with each outcome in the sequence.\n",
    "\n",
    "In this context, a higher entropy value suggests that the outcomes are more unpredictable or varied, while a lower entropy value indicates that the outcomes are more predictable or uniform.\n",
    "\n",
    "Here's what the entropy value tells us in this specific case:\n",
    "\n",
    "The entropy is measured in bits if using base-2 logarithm (which is commonly used in information theory).\n",
    "Each outcome in the sequence contributes to this average uncertainty.\n",
    "The closer the entropy value is to zero, the more predictable the outcomes are. Conversely, the closer it is to its maximum value (which depends on the number of possible outcomes), the more unpredictable the outcomes are.\n",
    "In this case, the entropy value of approximately 2.446 suggests that there is a moderate amount of uncertainty or variability in the sequence of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
